========================
Emerging Challenges
========================

The aforementioned dataset anonymization techniques can offer an appropriate level of privacy towards individuals in the data set. It is also clear that data set anonymization is superior with respect to privacy compared to pseudonymization. Moreover, metrics quantify the privacy level achieved in a transformed data set. However, many challenges remain. Some of them are discussed below.

First, multiple attacks can still be performed on anonymized data sets. A straightforward one is to enrich anonymized data with publicly available data (like data that is exposed on social networks). Doing so sometimes allows to re-identify certain individuals in an anonymized data set, or map records to individuals with a very high probability. Similarly, service providers can release another transformed version of the same original data set to different data processors. Although each transformed data set separately is not vulnerable to re-identification attacks, colluding data processors can do so by combining their view on the data set in some scenarios.

Second, applying solid generalizations can offer an appropriate level of privacy protection. However, doing so can undermine the utility of the transformed data set with respect to machine learning or optimization purposes. Data processors are no longer interested in useless transformed data. Therefore, privacy and utility must be balanced when applying data sets transformations. Note that -- like anonymity metrics – metrics exist to quantify the utility level of transformed datasets. Examples are discernability, ambiguity and classification metrics. However, practical experiments have shown that the widely recognized theoretic utility metrics often do not reflect the real value of the transformed dataset for a concrete purpose.

Third, the original sensitive data is often outsourced either (a) to external cloud providers that store and/or process the sensitive data on demand of the data owners, or (b) to software developers that need representative data to test their software that is built on demand of the data owners. Multiple techniques exist to protect the data towards honest-yet-curious service providers. For instance, data can be encrypted before it is sent to cloud providers. Homomorphic encryption schemes even allow service providers to execute algorithms on encrypted data. This typically comes with a serious performance penalty. Moreover, the data processing purpose must already be known in advance, which seriously impacts its flexibility. To meet the testing demands of external software developers, synthetic data can be provided. This allows to test software and scripts without privacy risks. Similarly, format-preserving encryption schemes avoid identifying data in test environments, thereby preserving the structure and format of the original data.

Finally, generating and releasing artificial data with similar statistical properties as the ones in the original data set is often proposed as an alternative to applying traditional anonymization techniques. Moreover, it is often argued that re-identification is no longer possible as the data is artificial. However, artificial data is often generated by a machine learning algorithm that is trained with the original data set, or a large subset. It is widely known and proven that machine learning algorithms often leak information about the training data. This means that re-identification attacks can be performed. Those attacks typically become more effective if more artificial data is released to an attacker. Moreover, it is no longer possible to apply the intuitive anonymity metrics – like k-anonymity – that can be applied to data sets that are transformed with traditional generalization strategies. This can seriously complicate the assessment of the (artificial) data set with respect to anonymity. 
